---
title: "Stats Overview"
output: pdf_document
toc: True
---


##Libraries Used
```{r,message=FALSE,results='hide'}
library(gmodels)
library(PSYC201)
library(ggplot2)
library(dplyr)
library(magrittr)
library(moments)
library(combinat)
library(gtools)

```
\pagebreak

#Permutation Rules 

Number of permutations with replacement, any possible order

 $$n^r$$ : $n$ is number of items to choose from, $r$ is number chosen- makes sense. 
 
Total Number of permutations without replacement 

$$\dfrac{n!}{(n-r)!}$$ -basically a function that gives you limit on declining number of possible options

Number of permuatations without replacement, but not requiring a specific order

$$\dfrac{n!}{r!(n-r)!}$$ - it is the above, but now we need to take into account the fact that the $r $ selected can come in any order.

#Univariate descriptive Statistics

##Definition of moments.

Point of clarification: Moments refers just the the set of numbers that describe a set of points, it makes sense in a physical system or to probability. They are defined according to the following function. 

$$\mu'_i = \int^b_a (x-c)^i f(x) dx$$

For probability $f(x)=p(x)$ so :

$$\mu'_i = \int^b_a (x-c)^i p(x) dx$$

As $i$ increases, you move up moments. The total number of moments as $i$ goes to $\inf$ completely describes a set of points. *Seems like you need might only need n moments, but what do I know*. In general, each moment is referred to as $\mu_i$, my guess is that using $\mu$ to represent mean is a convention to drop the 1. 



Zeroeth Moment- the probability mass function   
 
 $M_0 = \mu_0 = \Omega = 1$ 
 
First moment, the mean
  
\(M_1 = \mu_1 = \mu_{x} = \int^b_a (x-c)^1 f(x) dx =\int^b_a x f(x) dx  = E[X] \)     

Second Moment, variance. Note that $\sigma$ is the standard deviation.  
 
\(M_2 = \mu_2 = \sigma^2_x = \int^b_a (x-c)^2 f(x) dx  = E[(X-E[X])^2] = E[X^2] - E[X]^2 \)

Confirming with R
```{r,tidy=TRUE}
#need to adjust for sample calculation by subtracting 0, but not identitcal
x <- rnorm(200,0,4)
var(x)
sum((x^2))/(length(x)-1) - mean(x)^2
```
**Why don't I get the same results**


**Need to include a hardcoded function for evaluating variance of discrete function, maybe break it out for kurtosis and skew calculations**

This continues for the definition of moment, but using these in an non-normalized fashion doesn't appear normal


Third moment

\(M_3 = \mu_3 = \int^b_a (x-c)^3 f(x) dx \)  

Forth Moment

\(M_4 = \mu_4 = \int^b_a (x-c)^4 f(x) dx \)  

At this point, there now are what called "normal moments", where the are now "normliazed" by dividing by the  standard deviation. Now the $i_{th}$ moment is given by

$$\dfrac{\mu_i}{\sigma^i}$$

The third normalized or standardized moment is called Pearson moment coefficient of skeweness.


\(M_3 = \gamma = \dfrac{\mu_3}{\sigma^3} = E[ \left(\dfrac{s-\mu_1}{\sigma}\right)^3] = E[ \left(\dfrac{(E[(X-\mu)^3]}{E[(X-
\mu)^2]^\frac{3}{2}}\right) \)

Confirming with r
```{r, tidy=TRUE}
x <- rgamma(200,shape=10)
skewness(x)
mean(  mean( ( x- mean(x))^3) / ( mean(  (x - mean(x))^2 ) ^ (3/2)))
```

Forth Moment, kurtosis.

\(M_4 = \beta_2=\dfrac{\mu_4}{\sigma^4}  = E[ \left(\dfrac{(E[(X-\mu)^4]}{E[(X-
\mu)^2]^2}\right)]   \) 

Confirming with r
```{r, tidy=TRUE}
x <- rnorm(200)
kurtosis(x)
mean(  mean( (x - mean(x))^4 ) / ( mean(  (x - mean(x))^2 ) ^ (2))) 
```


However, because the kurtosis is equal to three in a normal distribution, is is often subtracted from the measure of kurtosis so that the sign of the value indictes whether a distribution is platykurtic (less than 0, low kurtosis) or leptokurtic ( greater than 0, high kurtosis). It is simultaneously a measure of three values. It generally indcreases with peakedness, how much variance is right next to the mean and tails and the tail thickness, but decreases with increased mass held in the sholders, or more modest deviations. However, it generally is seen as mostly a measure of tail thickness. 

\(M_4 = \beta_2=\dfrac{\mu_3}{\sigma^3}  = E[ \left(\dfrac{(E[(X-\mu)^4]}{E[(X-
mu)^2]^2}\right) -3]   \) 
\pagebreak



##Central Limit Theorem. 

Proof by assertion. It's an interesting idea. What happens when you start to sum n, iid variables. This is interesting as it assumes it observation is itself drawn from an independent distribution (which i guess is restating it's assumption). If you know the nature of the underlying distribution, you can calculate your expectation about the mean, variance, skew and kurtosis of the sum of the variables. 

Rules for adding or summing variables. 

$Mean(X+Y) = Mean(X) + Mean(Y)$  
$Mean(aX) = a * Mean(X)$  
$Var(X+Y) = Var(X) + Var(Y)$  
$Var(aX) = a^2 * Var(X)$  

Question:

Var(X+Y) = Var(X) + Var (Y)   
if Y = X   
why does Var(2*X) =! Var(X + X)? Because they are different transformations, doing something very different. This is a dumb question rob.

This means that if one starts to sum iid varables, n increases linearly. However, by assertion the kurtosis and skew start to fall towards zero, meaning that distribution of the sum of iid variables starts to look like a normal distribution. Apprently they increase like this, regardless of the initial distribution. 

$$Mean[\sum_{}^n X] = n * Mean[X]$$
$$Variance[\sum_{}^n X] = n * Variance[X]$$

$$skew(\sum_{}^n X) = \frac{1}{\sqrt{n}} * skew(x)$$

$$kurtosis(\sum^n X) = \frac{1}{n} * kurtosis(x)$$

This more or less underlies most statistics. There is the assumption that as you increase n, the expectation about the sum of the added random variables starts to resemble a normal variable (though normally you divide this value by n). This is cool.

**The actual Central limit theorem**

$P(\bar(X)_{n}) \rightarrow N(\mu_X, \frac{\sigma_X}{\sqrt{n}}$


## Normal variable 

Here X is distributed according to the following function.

$P(x|\mu,\sigma) = \dfrac{1}{\sqrt{2 \pi \sigma^2}} e^{ \left[- \dfrac{x-\mu}{2\sigma^2} \right]}$

Check in R
```{r,tidy=TRUE,fig.height=2}

gauss <- function(x, mu, sigma){
  const <- 1 / sqrt( 2 * pi * sigma^2)
  probability <- const * exp( - (x-mu)^2 / (2 * sigma^2) )
  return(probability)
} 

x<-1:600/100 - 3

qplot(x,gauss(x,0,1))
qplot(x,dnorm(x))
```



## Laws of large numbers 

If you have now taken the sum of this set of random variables, you can now get the expected value of the mean and variance by diving by n. I think this means that there is a careful set

* Strong Law of large numbers 

$$\bar{X} \rightarrow Mean[X]$$


* Weak Law of large numbers 

$$Prob(\bar{X}-\mu>e) \rightarrow 1$$


* Borels Law of large numbers

$$\dfrac{\sum{i=1}^n x}{n} \rightarrow P(X=x) $$

* Monte Carlo principle

$$\frac{1}{n} \sum{i=1}^nf(x^i) \rightarrow E_{P(x)}[f(X)]$$



## Multivariate extension

Fill in later


#Sampling Theory

##Expected Value of the mean

Knowing the following
$$Mean[\sum_{}^n X] = n * Mean[X]$$
$$Variance[\sum_{}^n X] = n * Variance[X]$$
$$skew(\sum_{}^n X) = \frac{1}{\sqrt{n}} * skew(x)$$
$$kurtosis(\sum^n X) = \frac{1}{n} * kurtosis(x)$$


What is the distribution we can expect when we sum X iid variables and divide by n. This is hte sampling distribution of the sample mean. 
Sample mean of n iid variables is is viewed as the distribution that results from the following.
$\dfrac{X_1+X_2+...+X_n}{n}$

So sum up the distributions, divide the sums by n, which gives us  the sampling distribution

$$Mean[\frac{1}{n} * \sum_{}^n X] = Mean[X]$$
$$Variance[\frac{1}{n} * \sum_{}^n X] = \frac{1}{n^2} * n * Variance[X] = \frac{1}{n} *Variance[X]$$

*If there is time, solve the following.* Right now, just take as an article of faith that they converge rapidly to the parametrs of a normal distribution.
$$skew(\frac{1}{n} * \sum_{}^n X) = ?$$
$$kurtosis(\frac{1}{n} * \sum^n X) = ?$$

What this means is the central limit theorem. 
$$P(\bar(X)_{n}) \rightarrow N(\mu_X, \frac{\sigma_X}{\sqrt{n}}$$

 
#Z (Standard Normal) distribution

The $N(0,1) distribution. Can transform distribution x to a z distribution with the following forumula 
$z_x = (x-\mu_x)/sd_x$ Subtract the population mean, divide by the population standard deviation. 

Particularly valuable because it allows one to compare different distributions. Often one looks at the z score transform of the sample mean, as it lets one run tests of it's value.

##pdf,cdf, icdf   

Here are the r commands, same for continuous and discrete probability density functions.   

```{r,fig.height=2,tidy=TRUE}
#pdf Probability mass function - what is the probability of this specific point, not really interpretable in a continuous distribution, but useful for discrete distributions.
x = 1:6000/1000 -3
qplot(x,dnorm(x))

#cdf Cumulative Density Function what is the probability of observing this this value or any value that is lower. This is inclusive the x value, which is important to remember for discrete distributions
x = 1:6000/1000 -3
qplot(x,pnorm(x))

#icdf,Inverse Cumulative Density function or quatile distribution. With this distribution, it returns the value such that p% of the probability distribution is lower than that value.
p <- 1:1000/ 1000
qplot(p, qnorm(p))


```

 -Not refer to law of total probability in the answers, particularly as numerator in bayes equation. 
 


#Null Hypothesis Sampling Testing.

## Confidence intervals
  
  - Identify the regions where there is an X% of the probability falls inside the value of that confidnece interval. 
  - Are weird and non-intuitive 
    + sample mean is random variable
    + population mean is fixed point value
    + can't say anything about probility distribution of population mean
    + can say- if we drew infinite sample of this size and calculated the 95%CI, then 95% of these intervals will cover sample mean
    + This is referred to as coverage, and frequentist statistics is optimized for this value.
- Frequentist procedures more concerned with procedure
- The Bayesian credible interval, under reasonable assumptions very closely resembles the confidence interval. 

Quick sanity check
```{r}
x <- rnorm(10000,3,5)
#95CI on the sample error
meanx <- 3
varx <- 5^2/10000
sdx <- 5/sqrt(10000) 
#lower bound
qnorm(.05/2,meanx,sdx)
#upper bound
meanx+(meanx-qnorm(.05/2,meanx,sdx))
ci(x)
```


## Significance testing.

Terms

Alpha - $\alpha$ - probability that you will observe outcomes more extreme than one's cuttoff for significance, conditional on the null hypothesis being true. Estimated frequency of Type 1 error.

Cohen's d = \(d = | \frac{\mu_T - \mu _0}{\sigma_X} | \) - measure effective size generally use $\bar{x}$ for $\mu_t$.

Power, or Beta - $\beta$ Probability of accepting rejecting null hypothesis, conditional on the alternative or tested hypothesis being true. Consequence of frequentist statistics. One identifies a value with alpha, where an observed value closer to the mean of the null hypothesis means the null hypothesis is accepted.

**Strategy for Calculating Power**

```{r,tidy=TRUE}

#null hypothesis parameters
mu0 <- 5
sd0 <- 5
#Alternative hypothesis parameters
muT <- 10

n <- 25
alpha = .05

#set up limits for null hpyothesis 
dif<- abs(mu0 - qnorm(alpha/2,mu0,sd0/sqrt(n)))
uplim  <- mu0+dif
lowlim <- mu0-dif

#now look at probability of seeing values outside that confidence interval, this is power

above<- 1- pnorm(uplim, muT,sd0/sqrt(n))
below<- pnorm(lowlim, muT,sd0/sqrt(n))

EstPower<-above+below



```

**Tricks for estimating power**

```{r,tidy=TRUE}
#check given above
d <- abs((mu0-muT)/sd0)


#power given effect size, it works but it is ignoring the probabiltiy that it will be rejected in the wrong way
pow = 1 - pnorm( abs(qnorm(alpha / 2)) - d*sqrt(n) )
pow == above
#number needed for n- this doesn't include "other sideded power, but maybe not important"
n.needed = ( abs( qnorm(alpha /2) - qnorm(pow) ) / d )^2

```

#T Test

##Necessity  

Basically using a systematically biased estimate of standard deviation leads to really wrong estimates of signifcance and is hard to correct. 

By Assertion, we start to correct this by estimating the population  variance as 

$$\hat{sigma}^2 = s^2 = \dfrac{1}{n-1} \sum_{i=1}^n (x_i-\hat{x})^2$$

Now the population standard devation is 

$$\hat{sigma} = s = \sqrt{\dfrac{1}{n-1} \sum_{i=1}^n (x_i-\hat{x})^2}$$

Then you could use the sample standard deviation (note, not the sample standard error) to calcualte z score

$$Z_x = \sqrt{n} \big( \dfrac{\bar{x}-\mu_0}{s} \big)$$

note this framing makes sense- you multiply by the square root of n, becuse the z score for the difference between the observed mean and the null mean increases with sample size.
```{r,tidy=TRUE}
x <- rnorm(10000,5,10)
sqrt(10000) * (5-0) / 10
(5-0) / (10 / sqrt(10000))
```

However, this doesn't work b/c the the z score calculated this way doesn't follow normal distribution, because the variance has sampling variation. This means statistics calculated from the z. What the above calculation actually gives you is the t score 

$$t_{\bar{x}} =\sqrt{n} \big( \dfrac{\bar{x}-\mu_0}{s_x} \big) == dfrac{\bar{x}-\mu_0}{s_x / \sqrt{n}}$$

You can see the problem if you hardcode the standard deviation, here at 1 as is dones in the z sample, vs estimate the standard devaition with the sd function. 

```{r,tidy=TRUE,fig.height=2}

z.sample = function(n){ 
x = rnorm(n, 0, 1); 
return( mean(x)/1*sqrt(n) ); 
} 

t.sample = function(n){ 
x = rnorm(n, 0, 1); 
return( mean(x)/sd(x)*sqrt(n) ); 
} 

qplot(replicate(100000, z.sample(4)))

qplot(replicate(100000, t.sample(4)))
```


##Degrees of Freedom.

Basically how may datapoints wer free when estimating the variance- If we knew the mean, standard deviation and the values of X points, at what point can we fully determine the values of the remain n-X points. X is standard deviation. For estimating variance X = n-1. Once we now n-1 values, the variance and the mean, we can fully determine the last value.

## Deteriming Sample Standard Deviation- not sample mean


### one sample 

-Already done


## two sample, equal variance

We have two samples and two estimates of variance. However, we have assued variance is the same, so now we need to pool our estimates of the variance. 

$$s_p^2 = \dfrac{(n_x-1)s_x^2 + (n_y -1)s_y^2}{n_x+n_y-2}$$

Where $n_x$ and $n_y$ are sample size, and $s^2$ is their respective sample variance, calculated according to:

$$\hat{\sigma}^2 = s^2 = \dfrac{1}{n-1} \sum_{i=1}^n (x_i-\hat{x})^2$$

Then we must use our earlier rules around adding variances to figure out how to identify the variance of the mean or the standard error. We are assuming that all the variables from each sample are distributed with possibly different means, but with the same variance. What we are trying go get the statistic of is what is the distribution of ? $\bar{x}- \bar{y}$, what is that value and what is its standard deviation. (standard error)

So go through previous experience w/r/t adding up 

$[x_1,....,x_n1] \sim P(X)$
$[y_1,....,y_n1] \sim P(X)$

$Mean[\bar{x}] = Mean[\bar{y}] =Mean[X]$


$Variance[\bar{x}] = \dfrac{1}{n_x} variance[X]$  

$Variance[\bar{y}] = \dfrac{1}{n_y} variance[X]$


$$ Mean[\bar{x}] - Mean[\bar{y}]= 0 $$

$$Variance[\bar{x}-\bar{y}]= \dfrac{1}{n_x}+ \dfrac{1}{n_y} + variance[X]$$

$$\sigma_{\bar{x}-\bar{y}}= \sigma{x} * \sqrt{\dfrac{1}{n_x}+ \dfrac{1}{n_y} }$$

Remember our smaple estimate of $\sigma_X$ is $s_p$.

Bringing it all together. 

$$t_{\bar{x}-\bar{y}} = \dfrac{\bar{x}-\bar{y}}{s_p} /  \sqrt{ \dfrac{1}{n_x} + \dfrac{1}{n_y}}$$

Also remember, you need to estimate two means, so df = $n_x+n_y-2$.  I was getting this wrong because I was being foolish about the standard deviation vs variance, I need to be really careful about that here.
```{r}

x1 = c(618,606,735,627,679,622,712,772,728,550,594,681,578,689,672)
x2 = c(571,569,613,693,714,521,530,736,677,626,722) 

n1= length(x1)
n2= length(x2)

varx1<- sum((x1-mean(x1))^2) / (n1-1)

varx2<- sum((x2-mean(x1))^2) / (n2-1)

SampleVar<- ((50-1)*(varx1) + (50-1)*(varx2) ) / (50+50-2)

SampleSD <- sqrt(SampleVar)

SampleMeanSD <- SampleSD / sqrt((1/50)+(1/50)) 

Tstat <- (mean(x1)-mean(x2)) / SampleSD / sqrt((1/50)+(1/50)) 

Tstat
#confirm with equal var ttest
t.test(x1,x2,var.equal=T)

```


##paired t test, 

-just run the subtraction and look at variance of the difference between pairs. Should be easy


## Two sample, unequal variance

Work out the math for two distributions drawn from same mean, but with different variances:

$[x_1,....,x_n1] \sim P(X)$
$[y_1,....,y_n1] \sim P(Y)$


$Mean[\bar{x}] = Mean[X] =Mean[ \bar{y} ] =  Mean[Y]$



$Variance[\bar{x}] = \dfrac{1}{n_x} variance[X]$  

$Variance[\bar{y}] = \dfrac{1}{n_y} variance[Y]$




$$ Mean[\bar{x}] - Mean[\bar{y}]= 0 $$

$$Variance[\bar{x}-\bar{y}]= \dfrac{1}{n_x} variance[X]+\dfrac{1}{n_y} variance[Y]$$

$$\sigma_{\bar{x}-\bar{y}}= \sqrt{ \dfrac{\sigma_X}{n_x}+\dfrac{\sigma_X}{n_x}}$$


For the t statistic
$$t_{\bar{x}-\bar{y}} = (\bar{x}-\bar{y}) / \sqrt{ \dfrac{s_{X}^2}{n_x}+\dfrac{s_{Y}^2}{n_y}}$$

-not putting in formual for calculating df, as its absurdly complicated, but you

r evaluation

```{r}

#sample 1 sd
s1 <- 5
#sample 1 n 
n1 <- 10

#sample 2 sd
s2 <- 2
#sample 2 n
n2<- 50

df = (s1^2/n1+s2^2/n2)^2/((s1^4/(n1^2*(n1-1)))+(s2^4/(n2^2*(n2-1))))

```

Putting it together
```{r, tidy=TRUE}
xD = c(25,11,16,13,33,21,26,31,14,22,31,10,26,26)
yD = c(-9,-19,16,18,46,8,30,45,25,33,11,5,23,22,38,32,-2)

m1 = mean(xD) 
m2 = mean(yD)

n1 = length(xD)
s1 = sd(xD)
n2 = length(yD)
s2 = sd(yD)

#t statistic
t = (m1-m2)/sqrt(s1^2/n1 + s2^2/n2)
t
# df 
df = (s1^2/n1+s2^2/n2)^2/((s1^4/(n1^2*(n1-1)))+(s2^4/(n2^2*(n2-1))))
df
#confirm with t test.
t.test(xD,yD)
```

#Binomail Distributions

##Details I'll probably add

## Normal Approximation

Define 

$\hat{p} = \frac{k}{n}$

where k  is number of success  and n is number of attempts

If n is big enough. 

- $\hat{p}$ will be normally distributed (central limit theorem)
- $\hat{p}$ is expected to equal p, it is an unbiased estimator
- $\hat{p}$ has variance $\frac{p*(1-p)}{N}$ - this is variance of the estimator
- $\hat{p}$ therefor has standard error of$\sqrt{frac{p*(1-p)}{N}}$. 
- so can run t tests on binomial data. 

##Categorical Data


#Statistics Ephemra

Quantile Quantile Plts 
```{r,tidy=TRUE,fig.height=2}
a<-rnorm(10000)+rnorm(10000)
a<-a^2
a1<-rnorm(10000)
b<-rt(10000,6)
qqplot(a,a1)
qqplot(a,b)
```



#Examples and Excercises

##Powerball Examples
Expected Value of Powerball lottery.
- Five white balls drawn without replacement from 59, order irrelevant
- red ball drawn from bin of 35
- $2 to play
- assume no other players

What is the expected value 

```{r}
#probability of getting 5 white balls
#five factorial 5 chances for first value, then 4 chances for second value, then..
#out of 59 chances for first value, 58 chances for second value.
white <- factorial(5) / prod((59-5):59)
red <- 1/ 35
Prob <- white*red

#does it work for simplier example 
#deck of 5 cards, draw three. need to get the three target values 1,2,3

#first draw could be any of the three
d1 <- 3/5
#second draw the same.
d2 <- 2/4
#third draw the same
d3 <- 1/3
p  <- d1*d2*d3
p

#this gives us correct answer- check with permuations function.
  
permutations(n=5,r=3,repeats.allowed=F) %>%
  data.frame %>%
  summarise(win = mean(X1+X2+X3==6))
```

#Questions 

-You don't use n-1 when calculating sd from sample, ofcourse not you have it 
- you also don't use n-1 
